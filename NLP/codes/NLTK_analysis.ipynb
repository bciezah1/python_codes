{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.warn('DelftStack')\n",
    "warnings.warn('Do not show this message')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reading document old style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my data In order to construct data pipelines and networks that stream, process, and store data, data engineers and data-science DevOps specialists must understand how to combine multiple big data technologies\n"
     ]
    }
   ],
   "source": [
    "with open(\"C:/Users/bciez/Documents/practice_folder/python_codes/NLP/input/Spark-Course-Description.txt\",'r') as fh:\n",
    "    filedata = fh.read()\n",
    "\n",
    "print('my data',filedata[0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reading document as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to construct data pipelines and networks that stream, process, and store data, data engineers and data-science DevOps specialists must understand how to combine multiple big data technologies. In this course, discover how to build big data pipelines around Apache Spark. Join Kumaran Ponnambalam as he takes you through how to make Apache Spark work with other big data technologies. He covers the basics of Apache Kafka Connect and how to integrate it with Spark for real-time streaming. In addition, he demonstrates how to use the various technologies to construct an end-to-end project that solves a real-world business problem.\n"
     ]
    }
   ],
   "source": [
    "corpus =  PlaintextCorpusReader(\"C:/Users/bciez/Documents/practice_folder/python_codes/NLP/input/\",\"Spark-Course-Description.txt\")\n",
    "print(corpus.raw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploring corpus properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in this corpus:  ['Spark-Course-Description.txt']\n",
      "\n",
      " total paragraphs in this corspus:  1\n",
      "\n",
      " total sentences in this corpus:  5\n",
      "\n",
      " the first sentence:  ['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', ',', 'process', ',', 'and', 'store', 'data', ',', 'data', 'engineers', 'and', 'data', '-', 'science', 'DevOps', 'specialists', 'must', 'understand', 'how', 'to', 'combine', 'multiple', 'big', 'data', 'technologies', '.']\n",
      "\n",
      " words in this corpus:  ['In', 'order', 'to', 'construct', 'data', 'pipelines', ...]\n"
     ]
    }
   ],
   "source": [
    "# exploring corpus\n",
    "\n",
    "print(\"Files in this corpus: \",corpus.fileids())\n",
    "\n",
    "# extract paragraphs\n",
    "paragraphs = corpus.paras()\n",
    "print('\\n total paragraphs in this corspus: ', len(paragraphs))\n",
    "\n",
    "# extract sentences\n",
    "sentences = corpus.sents()\n",
    "print('\\n total sentences in this corpus: ',len(sentences))\n",
    "print('\\n the first sentence: ',sentences[0])\n",
    "\n",
    "# extract words\n",
    "print('\\n words in this corpus: ',corpus.words())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Analyzing our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'to': 8, 'data': 7, ',': 5, '-': 5, 'how': 5, '.': 5, 'and': 4, 'In': 3, 'big': 3, 'technologies': 3, ...})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_freq_dist = nltk.FreqDist(corpus.words())\n",
    "corpus_freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 words in the corpus:  [('to', 8), ('data', 7), (',', 5), ('-', 5), ('how', 5), ('.', 5), ('and', 4), ('In', 3), ('big', 3), ('technologies', 3)]\n",
      "\n",
      " distribution for \"Spark\" : 3\n"
     ]
    }
   ],
   "source": [
    "print('top 10 words in the corpus: ',corpus_freq_dist.most_common(10))\n",
    "print('\\n distribution for \\\"Spark\\\" :',corpus_freq_dist.get('Spark'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Tockenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In order to construct data pipelines and networks that stream, process, and store data, data engineers and data-science DevOps specialists must understand how to combine multiple big data technologies. In this course, discover how to build big data pipelines around Apache Spark. Join Kumaran Ponnambalam as he takes you through how to make Apache Spark work with other big data technologies. He covers the basics of Apache Kafka Connect and how to integrate it with Spark for real-time streaming. In addition, he demonstrates how to use the various technologies to construct an end-to-end project that solves a real-world business problem.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_file = open(\"C:/Users/bciez/Documents/practice_folder/python_codes/NLP/input/Spark-Course-Description.txt\",'rt')\n",
    "raw_text = base_file.read()\n",
    "base_file.close()\n",
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list:  ['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', ',', 'process', ',', 'and', 'store', 'data', ',', 'data', 'engineers', 'and']\n",
      "\n",
      " total tokens:  110\n"
     ]
    }
   ],
   "source": [
    "token_list =  nltk.word_tokenize(raw_text)\n",
    "print('token list: ', token_list[0:20])\n",
    "print('\\n total tokens: ', len(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list afger removing puncutation:  ['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', 'process', 'and', 'store', 'data', 'data', 'engineers', 'and', 'data-science', 'DevOps', 'specialists']\n",
      "\n",
      " total token after removing punctuation:  100\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation\n",
    "\n",
    "token_list2 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list))\n",
    "print('token list afger removing puncutation: ', token_list2[0:20])\n",
    "print('\\n total token after removing punctuation: ',len(token_list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', 'process', 'and', 'store', 'data', 'data', 'engineers', 'and', 'data-science', 'devops', 'specialists']\n",
      "\n",
      " total tokents 100\n"
     ]
    }
   ],
   "source": [
    "# converting to lower cases\n",
    "\n",
    "token_list3 = [word.lower() for word in token_list2]\n",
    "print(token_list3[0:20])\n",
    "print(\"\\n total tokents\",len(token_list3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list ['order', 'construct', 'data', 'pipelines', 'networks', 'stream', 'process', 'store', 'data', 'data', 'engineers', 'data-science', 'devops', 'specialists', 'must', 'understand', 'combine', 'multiple', 'big', 'data']\n",
      "\n",
      " total tokens after removing stop words:  62\n"
     ]
    }
   ],
   "source": [
    "# removing stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "token_list4 =  list(filter(lambda token: token not in stopwords.words('english'),token_list3))\n",
    "print('token list', token_list4[0:20])\n",
    "print('\\n total tokens after removing stop words: ', len(token_list4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list  ['order', 'construct', 'data', 'pipelin', 'network', 'stream', 'process', 'store', 'data', 'data', 'engin', 'data-sci', 'devop', 'specialist', 'must', 'understand', 'combin', 'multipl', 'big', 'data']\n",
      "\n",
      " total tokens after stemming:  62\n"
     ]
    }
   ],
   "source": [
    "# stemming word list\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "token_list5 =  [stemmer.stem(word) for word in token_list4]\n",
    "print('token list ',token_list5[0:20])\n",
    "print('\\n total tokens after stemming: ', len(token_list5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token list after lematizing:  ['order', 'construct', 'data', 'pipeline', 'network', 'stream', 'process', 'store', 'data', 'data', 'engineer', 'data-science', 'devops', 'specialist', 'must', 'understand', 'combine', 'multiple', 'big', 'data']\n",
      "\n",
      " total token after lematization:  62\n"
     ]
    }
   ],
   "source": [
    "# lemmatizatizing wods list\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "token_list6 = [lemmatizer.lemmatize(word) for word in token_list4]\n",
    "\n",
    "print('token list after lematizing: ', token_list6[0:20])\n",
    "print('\\n total token after lematization: ',len(token_list6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most commong bigrams: \n",
      "[(('big', 'data'), 3), (('data', 'pipeline'), 2), (('data', 'technology'), 2), (('apache', 'spark'), 2), (('order', 'construct'), 1)]\n",
      "most commong bigrams: \n",
      "[(('big', 'data', 'technology'), 2), (('order', 'construct', 'data'), 1), (('construct', 'data', 'pipeline'), 1), (('data', 'pipeline', 'network'), 1), (('pipeline', 'network', 'stream'), 1)]\n"
     ]
    }
   ],
   "source": [
    "# calc n-grams from my processed text\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "bigrams = ngrams(token_list6,2)\n",
    "print('most commong bigrams: ')\n",
    "print(Counter(bigrams).most_common(5))\n",
    "\n",
    "trigrams = ngrams(token_list6,3)\n",
    "print('most commong bigrams: ')\n",
    "print(Counter(trigrams).most_common(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order', 'NN'),\n",
       " ('construct', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('pipelines', 'NNS'),\n",
       " ('networks', 'NNS'),\n",
       " ('stream', 'VBP'),\n",
       " ('process', 'NN'),\n",
       " ('store', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('data', 'NNS')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS TAG\n",
    "\n",
    "nltk.pos_tag(token_list4)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token used as features are: \n",
      "['america', 'basketbal', 'basketball', 'league', 'nba', 'popular', 'telecast', 'tv']\n",
      "\n",
      " size of array\n",
      "(3, 8)\n",
      "\n",
      " actual tf-idf array\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.4736296 , 0.62276601, 0.62276601,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.51785612, 0.        , 0.51785612, 0.        , 0.        ,\n",
       "        0.68091856, 0.        , 0.        ],\n",
       "       [0.40204024, 0.52863461, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.52863461, 0.52863461]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# term frequency-inverse document frequency\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "vector_corpus = ['NBA is a Basketball league',\n",
    "                 'Basketball is popular in America',\n",
    "                 'TV in America telecast Basketbal']\n",
    "\n",
    "vectorizer =  TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# create vector\n",
    "tfidf =  vectorizer.fit_transform(vector_corpus)\n",
    "\n",
    "print('token used as features are: ')\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print('\\n size of array')\n",
    "print(tfidf.shape)\n",
    "\n",
    "print('\\n actual tf-idf array')\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
