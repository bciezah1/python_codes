{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.warn('DelftStack')\n",
    "warnings.warn('Do not show this message')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reading document old style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my data In order to construct data pipelines and networks that stream, process, and store data, data engineers and data-science DevOps specialists must understand how to combine multiple big data technologies\n"
     ]
    }
   ],
   "source": [
    "with open(\"C:/Users/bciez/Documents/practice_folder/python_codes/NLP/input/Spark-Course-Description.txt\",'r') as fh:\n",
    "    filedata = fh.read()\n",
    "\n",
    "print('my data',filedata[0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reading document as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to construct data pipelines and networks that stream, process, and store data, data engineers and data-science DevOps specialists must understand how to combine multiple big data technologies. In this course, discover how to build big data pipelines around Apache Spark. Join Kumaran Ponnambalam as he takes you through how to make Apache Spark work with other big data technologies. He covers the basics of Apache Kafka Connect and how to integrate it with Spark for real-time streaming. In addition, he demonstrates how to use the various technologies to construct an end-to-end project that solves a real-world business problem.\n"
     ]
    }
   ],
   "source": [
    "corpus =  PlaintextCorpusReader(\"C:/Users/bciez/Documents/practice_folder/python_codes/NLP/input/\",\"Spark-Course-Description.txt\")\n",
    "print(corpus.raw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploring corpus properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in this corpus:  ['Spark-Course-Description.txt']\n",
      "\n",
      " total paragraphs in this corspus:  1\n",
      "\n",
      " total sentences in this corpus:  5\n",
      "\n",
      " the first sentence:  ['In', 'order', 'to', 'construct', 'data', 'pipelines', 'and', 'networks', 'that', 'stream', ',', 'process', ',', 'and', 'store', 'data', ',', 'data', 'engineers', 'and', 'data', '-', 'science', 'DevOps', 'specialists', 'must', 'understand', 'how', 'to', 'combine', 'multiple', 'big', 'data', 'technologies', '.']\n",
      "\n",
      " words in this corpus:  ['In', 'order', 'to', 'construct', 'data', 'pipelines', ...]\n"
     ]
    }
   ],
   "source": [
    "# exploring corpus\n",
    "\n",
    "print(\"Files in this corpus: \",corpus.fileids())\n",
    "\n",
    "# extract paragraphs\n",
    "paragraphs = corpus.paras()\n",
    "print('\\n total paragraphs in this corspus: ', len(paragraphs))\n",
    "\n",
    "# extract sentences\n",
    "sentences = corpus.sents()\n",
    "print('\\n total sentences in this corpus: ',len(sentences))\n",
    "print('\\n the first sentence: ',sentences[0])\n",
    "\n",
    "# extract words\n",
    "print('\\n words in this corpus: ',corpus.words())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Analyzing our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'to': 8, 'data': 7, ',': 5, '-': 5, 'how': 5, '.': 5, 'and': 4, 'In': 3, 'big': 3, 'technologies': 3, ...})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_freq_dist = nltk.FreqDist(corpus.words())\n",
    "corpus_freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 words in the corpus:  [('to', 8), ('data', 7), (',', 5), ('-', 5), ('how', 5), ('.', 5), ('and', 4), ('In', 3), ('big', 3), ('technologies', 3)]\n",
      "\n",
      " distribution for \"Spark\" : 3\n"
     ]
    }
   ],
   "source": [
    "print('top 10 words in the corpus: ',corpus_freq_dist.most_common(10))\n",
    "print('\\n distribution for \\\"Spark\\\" :',corpus_freq_dist.get('Spark'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
